#Pre-requisite: hadoop-3.x

#Install Scala
sudo apt install scala
scala -version

#Download Spark
cd ~/Downloads
curl https://archive.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz -o spark-2.4.3.tgz
tar xzvf spark-2.4.3.tgz

sudo mv spark-2.4.3-bin-hadoop2.7 /opt/spark

#SPARK_HOME configuration
sudo nano ~/.bashrc
#############################
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/sbin:$SPARK_HOME/bin 
##############################################
source ~/.bashrc			=> reloads the changes

#Start Master
start-master.sh				=> (Web URL: http://localhost:8080/)

#Start Worker
start-slave.sh spark://bigdata-box:7077

start-slave.sh spark://ip-172-31-2-250.ap-south-1.compute.internal:7077

#Launch Spark Scala Interactive Shell
spark-shell

#Launch Spark Scala Interactive Shell Standalone cluster
spark-shell --master spark://bigdata-box:7077

spark-shell --master spark://ip-172-31-2-250.ap-south-1.compute.internal:7077

#WordCount Example (Execute in spark-shell)
val inputfile = sc.textFile("/home/ubuntu/danskeit_bigdata/labs/dataset/wordcount/wordcount-input.txt")
val counts = inputfile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_);
counts.toDebugString
counts.cache()
counts.saveAsTextFile("/home/ubuntu/output")